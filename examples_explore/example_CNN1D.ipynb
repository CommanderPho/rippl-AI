{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN1D parameter exploration\n",
    "This notebook is a template for finding the CNN1D model best suited for your needs\n",
    "\n",
    "This architecture is based in the CNN1D designed by A. Navas-Oliv√© (https://doi.org/10.7554/eLife.77772).\n",
    "This arquitecture is inspired by the UNet (https://doi.org/10.48550/arXiv.1505.04597) and YOLOR (https://doi.org/10.48550/arXiv.2105.04206)\n",
    "The 1st half uses convolution and MaxPooling to reduce the dimnensinality of the input, and the late half expands it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'rippl_AI' from 'c:\\\\SWR_repo\\\\rippl_AI.py'>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import importlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "parent_dir=os.path.dirname(os.getcwd())\n",
    "sys.path.insert(0,parent_dir)\n",
    "import rippl_AI\n",
    "import aux_fcn\n",
    "importlib.reload(aux_fcn)\n",
    "importlib.reload(rippl_AI)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data download\n",
    "4 uLED sessions will be downloaded: Amigo2 and Som2 will be used for training ; Dlx1 and Thy7 for validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from figshare.figshare.figshare import Figshare\n",
    "fshare = Figshare()\n",
    "\n",
    "article_ids = [16847521,16856137,14959449,14960085] \n",
    "sess=['Amigo2','Som2','Dlx1','Thy7']                                  \n",
    "for id,s in zip(article_ids,sess):\n",
    "    datapath = os.path.join(parent_dir,'Downloaded_data', f'{s}')\n",
    "    if os.path.isdir(datapath):\n",
    "        print(f\"{s} session already exists. Moving on.\")\n",
    "    else:\n",
    "        print(\"Downloading data... Please wait, this might take up some time\")        # Can take up to 10 minutes\n",
    "        fshare.retrieve_files_from_article(id,directory=datapath)\n",
    "        print(\"Data downloaded!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data load\n",
    "The training sessions' LFP will be appended together in a list. The same will happen with the ripples detection times.\n",
    "That is the required input for the training parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\SWR_repo\\Downloaded_data\\Amigo2\\figshare_16847521/hippo_2019-07-11_11-57-07_1150um_shank3.dat\n",
      "fileStart  0\n",
      "fileStop  1151451136\n",
      "nSamples  575725568\n",
      "nSamplesPerChannel  71965696\n",
      "nSamplesPerChunk  10000\n",
      "size data  71965696\n",
      "c:\\SWR_repo\\Downloaded_data\\Som2\\figshare_16856137/hippo_2019-07-24_12-01-49_1530um_shank3.dat\n",
      "fileStart  0\n",
      "fileStop  497401856\n",
      "nSamples  248700928\n",
      "nSamplesPerChannel  31087616\n",
      "nSamplesPerChunk  10000\n",
      "size data  31087616\n",
      "c:\\SWR_repo\\Downloaded_data\\Dlx1\\figshare_14959449/lfp_Dlx1-2021-02-12_12-46-54.dat\n",
      "fileStart  0\n",
      "fileStop  490242048\n",
      "nSamples  245121024\n",
      "nSamplesPerChannel  30640128\n",
      "nSamplesPerChunk  10000\n",
      "size data  30640128\n",
      "c:\\SWR_repo\\Downloaded_data\\Thy7\\figshare_14960085/lfp_Thy7-2020-11-11_16-05-00.dat\n",
      "fileStart  0\n",
      "fileStop  357220352\n",
      "nSamples  178610176\n",
      "nSamplesPerChannel  22326272\n",
      "nSamplesPerChunk  10000\n",
      "size data  22326272\n",
      "Original training data shape:  (71965696, 8)\n",
      "Downsampling data at 1250 Hz...\n",
      "Shape of downsampled data: (2998571, 8)\n",
      "Normalizing data...\n",
      "Original training data shape:  (31087616, 8)\n",
      "Downsampling data at 1250 Hz...\n",
      "Shape of downsampled data: (1295317, 8)\n",
      "Normalizing data...\n",
      "Original validation data shape:  (30640128, 8)\n",
      "Downsampling data at 1250 Hz...\n",
      "Shape of downsampled data: (1276672, 8)\n",
      "Normalizing data...\n",
      "Original validation data shape:  (22326272, 8)\n",
      "Downsampling data at 1250 Hz...\n",
      "Shape of downsampled data: (930261, 8)\n",
      "Normalizing data...\n"
     ]
    }
   ],
   "source": [
    "# The training sessions will be appended together. Replace this cell with your own data loading\n",
    "train_LFPs=[]\n",
    "train_GTs=[]\n",
    "# Amigo2\n",
    "path=os.path.join(parent_dir,'Downloaded_data','Amigo2','figshare_16847521')\n",
    "LFP,GT=aux_fcn.load_lab_data(path)\n",
    "train_LFPs.append(LFP)\n",
    "train_GTs.append(GT)\n",
    "# Som2\n",
    "path=os.path.join(parent_dir,'Downloaded_data','Som2','figshare_16856137')\n",
    "LFP,GT=aux_fcn.load_lab_data(path)\n",
    "train_LFPs.append(LFP)\n",
    "train_GTs.append(GT)\n",
    "\n",
    "## Append all your validation sessions\n",
    "val_LFPs=[]\n",
    "val_GTs=[]\n",
    "# Dlx1 Validation\n",
    "path=os.path.join(parent_dir,'Downloaded_data','Dlx1','figshare_14959449')\n",
    "LFP,GT=aux_fcn.load_lab_data(path)\n",
    "val_LFPs.append(LFP)\n",
    "val_GTs.append(GT)\n",
    "# Thy07 Validation\n",
    "path=os.path.join(parent_dir,'Downloaded_data','Thy7','figshare_14960085')\n",
    "LFP,GT=aux_fcn.load_lab_data(path)\n",
    "val_LFPs.append(LFP)\n",
    "val_GTs.append(GT)\n",
    "\n",
    "x_training,GT_training,x_val_list,GT_val_list=rippl_AI.prepare_training_data(train_LFPs,train_GTs,val_LFPs,val_GTs,sf=30000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN1D training parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters:\n",
    "* Channels:  number of channels that will be used to train the model, extracted from the data shape defined in the previous cell\n",
    "* Timesteps: number of samples that the model will use to produce a single output\n",
    "* Configuration: list with as many elements as layers in the model shaped [number of kernels layers, kernel size and stride ]. The length size and the kernel layer were matched to reduce design complexity.\n",
    "* Epoch: number of times the training data set is used to train the model\n",
    "* Training batch: number of windows that are proccessed before weight updating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf= {\"timesteps\":   [16],        # 16, 32, 64 ...\n",
    "      \"configuration\":      [[[4,2],[2,1],[8,2],[4,1],[16,2],[8,1],[32,2]],  \n",
    "                     [[4,4],[2,1],[8,2],[4,1],[16,2],[8,1],[32,2]]],  \n",
    "      \"epochs\":      [1],         # 1, 2, 3, 5...\n",
    "      \"train_batch\": [2**5],      # 32, 64, 128...\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 channels will be used to train the CNN1D models\n",
      "2 models will be trained\n",
      "Train Input and Output dimension (220312, 16, 8) (220312, 1)\n",
      "\n",
      "Iteration 1 out of 2\n",
      "Number of channels: 8, Time steps: 16,\n",
      "configuration: [[4, 2], [2, 1], [8, 2], [4, 1], [16, 2], [8, 1], [32, 2]]\n",
      "Epochs: 1, Samples per batch: 32\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 59\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mIteration \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39miter\u001b[39m\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m out of \u001b[39m\u001b[39m{\u001b[39;00mn_iters\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     57\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mNumber of channels: \u001b[39m\u001b[39m{\u001b[39;00mn_channels\u001b[39m:\u001b[39;00m\u001b[39md\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Time steps: \u001b[39m\u001b[39m{\u001b[39;00mtimesteps\u001b[39m:\u001b[39;00m\u001b[39md\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m,\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mconfiguration: \u001b[39m\u001b[39m{\u001b[39;00mconfiguration\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mEpochs: \u001b[39m\u001b[39m{\u001b[39;00mepochs\u001b[39m:\u001b[39;00m\u001b[39md\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Samples per batch: \u001b[39m\u001b[39m{\u001b[39;00mtrain_batch\u001b[39m:\u001b[39;00m\u001b[39md\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 59\u001b[0m model \u001b[39m=\u001b[39m aux_fcn\u001b[39m.\u001b[39;49mbuild_CNN1D(n_channels,timesteps,configuration)\n\u001b[0;32m     60\u001b[0m \u001b[39m# Training\u001b[39;00m\n\u001b[0;32m     61\u001b[0m model\u001b[39m.\u001b[39mfit(x_train, y_train,shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, epochs\u001b[39m=\u001b[39mepochs,batch_size\u001b[39m=\u001b[39mtrain_batch,validation_data\u001b[39m=\u001b[39m(x_test,y_test), verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\SWR_repo\\aux_fcn.py:1047\u001b[0m, in \u001b[0;36mbuild_CNN1D\u001b[1;34m(n_channels, timesteps, conf)\u001b[0m\n\u001b[0;32m   1045\u001b[0m \t\to_shape_arr\u001b[39m.\u001b[39mappend(timesteps\u001b[39m/\u001b[39m\u001b[39m/\u001b[39mlayer_conf[\u001b[39m1\u001b[39m])\n\u001b[0;32m   1046\u001b[0m \t\u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1047\u001b[0m \t\to_shape_arr\u001b[39m.\u001b[39mappend(o_shape_arr[i]\u001b[39m/\u001b[39m\u001b[39m/\u001b[39mlayer_conf[i]) \u001b[39m# The output shape of the layer n is the output of the n-1 layer /stride\u001b[39;00m\n\u001b[0;32m   1048\u001b[0m \u001b[39m#assert 0 not in o_shape_arr, \"\\nThe chosen kernel dimensionality will cause problems.\\nTry one of the following:\\n1. Increment time window (timesteps) size.\\n2. Decrement the 1st kernel size.\\n3. Decrement the rest of the kernels size\"\u001b[39;00m\n\u001b[0;32m   1050\u001b[0m \u001b[39massert\u001b[39;00m o_shape_arr[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m==\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mThe output of the model is not shaped (1,1).\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mTry incrementing the kernel size of the final layers\u001b[39m\u001b[39m'\u001b[39m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Desired sampling frequency of the models\n",
    "sf=1250\n",
    "th_arr=np.linspace(0.1,0.9,9)\n",
    "model_name_arr=[]           # To plot in the next cell\n",
    "model_arr=[]                # Actual model array, used in the next validation section\n",
    "n_channels=x_training.shape[1]\n",
    "timesteps_arr=conf['timesteps']\n",
    "\n",
    "config_arr=conf['configuration']\n",
    "epochs_arr=conf['epochs']\n",
    "train_batch_arr=conf['train_batch']   \n",
    "\n",
    "l_ts=len(timesteps_arr)\n",
    "l_conf=len(config_arr)\n",
    "l_epochs =len(epochs_arr)\n",
    "l_batch =len(train_batch_arr)\n",
    "n_iters=l_ts*l_conf*l_epochs*l_batch\n",
    "# GT is in the shape (n_events x 2), a y output signal with the same length as x is required\n",
    "perf_train_arr=np.zeros(shape=(n_iters,len(th_arr),3)) # Performance array, (n_models x n_th x 3 ) [P R F1]\n",
    "perf_test_arr=np.zeros_like(perf_train_arr)\n",
    "timesteps_arr_ploting=[]            # Array that will be used in the validation, to be able to call the function predict\n",
    "\n",
    "print(f'{n_channels} channels will be used to train the CNN1D models')\n",
    "\n",
    "print(f'{n_iters} models will be trained')\n",
    "\n",
    "x_test_or,GT_test,x_train_or,GT_train=aux_fcn.split_data(x_training,GT_training,split=0.7,sf=sf)\n",
    "\n",
    "y_test_or= np.zeros(shape=(len(x_test_or)))\n",
    "for ev in GT_test:\n",
    "    y_test_or[int(sf*ev[0]):int(sf*ev[1])]=1\n",
    "y_train_or= np.zeros(shape=(len(x_train_or)))\n",
    "for ev in GT_train:\n",
    "    y_train_or[int(sf*ev[0]):int(sf*ev[1])]=1\n",
    "\n",
    "\n",
    "for i_ts,timesteps in enumerate(timesteps_arr):\n",
    "    x_train=x_train_or[:len(x_train_or)-len(x_train_or)%timesteps].reshape(-1,timesteps,n_channels)\n",
    "    y_train_aux=y_train_or[:len(y_train_or)-len(y_train_or)%timesteps].reshape(-1,timesteps)\n",
    "    x_test=x_test_or[:len(x_test_or)-len(x_test_or)%timesteps].reshape(-1,timesteps,n_channels)\n",
    "    y_test_aux=y_test_or[:len(y_test_or)-len(y_test_or)%timesteps].reshape(-1,timesteps)\n",
    "\n",
    "    y_train=np.zeros(shape=[x_train.shape[0],1])\n",
    "    for i in range(y_train_aux.shape[0]):\n",
    "        y_train[i]=1  if any (y_train_aux[i]==1) else 0\n",
    "    print(\"Train Input and Output dimension\", x_train.shape,y_train.shape)\n",
    "    \n",
    "    y_test=np.zeros(shape=[x_test.shape[0],1])\n",
    "    for i in range(y_test_aux.shape[0]):\n",
    "        y_test[i]=1  if any (y_test_aux[i]==1) else 0\n",
    "\n",
    "    for i_conf, configuration in enumerate(config_arr):\n",
    "        for i_epochs,epochs in enumerate(epochs_arr):\n",
    "            for i_batch,train_batch in enumerate(train_batch_arr):\n",
    "                iter=((i_ts*l_conf+i_conf)*l_epochs + i_epochs)*l_batch + i_batch\n",
    "                print(f\"\\nIteration {iter+1} out of {n_iters}\")\n",
    "                print(f'Number of channels: {n_channels:d}, Time steps: {timesteps:d},\\nconfiguration: {configuration}\\nEpochs: {epochs:d}, Samples per batch: {train_batch:d}')\n",
    "\n",
    "                model = aux_fcn.build_CNN1D(n_channels,timesteps,configuration)\n",
    "                # Training\n",
    "                model.fit(x_train, y_train,shuffle=False, epochs=epochs,batch_size=train_batch,validation_data=(x_test,y_test), verbose=1)\n",
    "                model_arr.append(model)\n",
    "                # Prediction\n",
    "                test_signal = model.predict(x_test,verbose=1)\n",
    "                train_signal=model.predict(x_train,verbose=1)\n",
    "\n",
    "                y_train_predict=np.empty(shape=(x_train.shape[0]*timesteps,1,1))\n",
    "                for i,window in enumerate(train_signal):\n",
    "                    y_train_predict[i*timesteps:(i+1)*timesteps]=window\n",
    "                y_test_predict=np.empty(shape=(x_test.shape[0]*timesteps,1,1))\n",
    "                for i,window in enumerate(test_signal):\n",
    "                    y_test_predict[i*timesteps:(i+1)*timesteps]=window\n",
    "\n",
    "                ############################\n",
    "                for i,th in enumerate(th_arr):\n",
    "                    # Test\n",
    "                    ytest_pred_ind=aux_fcn.get_predictions_index(y_test_predict,th)/sf\n",
    "                    perf_test_arr[iter,i]=aux_fcn.get_performance(ytest_pred_ind,GT_test,0)[0:3]\n",
    "                    # Train\n",
    "                    ytrain_pred_ind=aux_fcn.get_predictions_index(y_train_predict,th)/sf\n",
    "                    perf_train_arr[iter,i]=aux_fcn.get_performance(ytrain_pred_ind,GT_train,0)[0:3]\n",
    "\n",
    "                    # Saving the model\n",
    "                    model_name=f\"CNN1D_Ch{n_channels:d}_Ts{timesteps:03d}_C{i_conf:02d}_E{epochs:02d}_TB{train_batch:04d}\"\n",
    "                    model.save(os.path.join(parent_dir,'explore_models',model_name))\n",
    "\n",
    "                    model_name_arr.append(model_name)\n",
    "                    timesteps_arr_ploting.append(timesteps)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m fig,axs\u001b[39m=\u001b[39mplt\u001b[39m.\u001b[39msubplots(n_iters,\u001b[39m2\u001b[39m,figsize\u001b[39m=\u001b[39m(\u001b[39m10\u001b[39m,\u001b[39m2\u001b[39m\u001b[39m*\u001b[39mn_iters),sharey\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcol\u001b[39m\u001b[39m'\u001b[39m,sharex\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcol\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_iters):\n\u001b[1;32m----> 5\u001b[0m     axs[i,\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39mplot(perf_train_arr[i,:,\u001b[39m0\u001b[39m],perf_train_arr[i,:,\u001b[39m1\u001b[39m],\u001b[39m'\u001b[39m\u001b[39mk.-\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m     axs[i,\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mplot(perf_test_arr[i,:,\u001b[39m0\u001b[39m],perf_test_arr[i,:,\u001b[39m1\u001b[39m],\u001b[39m'\u001b[39m\u001b[39mb.-\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m     axs[i,\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mplot(th_arr,perf_train_arr[i,:,\u001b[39m2\u001b[39m],\u001b[39m'\u001b[39m\u001b[39mk.-\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training results\n",
    "fig,axs=plt.subplots(n_iters,2,figsize=(10,2*n_iters),sharey='col',sharex='col')\n",
    "\n",
    "for i in range(n_iters):\n",
    "    axs[i,0].plot(perf_train_arr[i,:,0],perf_train_arr[i,:,1],'k.-')\n",
    "    axs[i,0].plot(perf_test_arr[i,:,0],perf_test_arr[i,:,1],'b.-')\n",
    "    axs[i,1].plot(th_arr,perf_train_arr[i,:,2],'k.-')\n",
    "    axs[i,1].plot(th_arr,perf_test_arr[i,:,2],'b.-')\n",
    "    axs[i,0].set_title(model_name_arr[i])\n",
    "    axs[i,0].set_ylabel('Precision')\n",
    "    axs[i,1].set_ylabel('F1')\n",
    "axs[-1,0].set_xlabel('Recall')\n",
    "axs[-1,1].set_xlabel('Threshold')\n",
    "axs[0,0].legend(['Training','Test'])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For loop iterating over the models\n",
    "fig,axs=plt.subplots(n_iters,2,figsize=(10,2*n_iters),sharey='col',sharex='col')\n",
    "for n_m,model in enumerate(model_arr):\n",
    "    F1_arr=np.zeros(shape=(len(x_val_list),len(th_arr))) #(n_val_sess x n_th) Array where the F1 val of each sesion will be stored\n",
    "    for n_sess,LFP in enumerate(x_val_list):\n",
    "        val_pred=rippl_AI.predict(LFP,sf=1250,arch='CNN1D',new_model=model,n_channels=n_channels,n_timesteps=timesteps_arr_ploting[n_m])[0]\n",
    "        for i,th in enumerate(th_arr):\n",
    "            val_pred_ind=aux_fcn.get_predictions_index(val_pred,th)/sf\n",
    "            F1_arr[n_sess,i]=aux_fcn.get_performance(val_pred_ind,GT_val_list[n_sess],verbose=False)[2]\n",
    "    \n",
    "    axs[n_m,0].plot(th_arr,perf_train_arr[n_m,:,2],'k.-')\n",
    "    axs[n_m,0].plot(th_arr,perf_test_arr[n_m,:,2],'b.-')\n",
    "    for F1 in F1_arr:\n",
    "        axs[n_m,1].plot(th_arr,F1)\n",
    "    axs[n_m,1].plot(th_arr,np.mean(F1_arr,axis=0),'k.-')\n",
    "    axs[n_m,0].set_title(model_name_arr[n_m])\n",
    "    axs[n_m,0].set_ylabel('Precision')\n",
    "    axs[n_m,1].set_ylabel('F1')\n",
    "axs[-1,0].set_xlabel('Recall')\n",
    "axs[-1,1].set_xlabel('Threshold')\n",
    "plt.show()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PublicBCG_d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
