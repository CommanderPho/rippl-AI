{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM parameter exploration\n",
    "This notebook is a template for finding the SVM model best suited for your needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import importlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "from sklearn import svm,calibration\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import sys\n",
    "import inspect\n",
    "parent_dir=os.path.dirname(os.getcwd())\n",
    "sys.path.insert(0,parent_dir )\n",
    "import rippl_AI\n",
    "import aux_fcn\n",
    "importlib.reload(aux_fcn)\n",
    "importlib.reload(rippl_AI)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data download\n",
    "4 uLED sessions will be downloaded: Amigo2 and Som2 will be used for training ; Dlx1 and Thy7 for validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from figshare.figshare.figshare import Figshare\n",
    "fshare = Figshare()\n",
    "\n",
    "article_ids = [16847521,16856137,14959449,14960085] \n",
    "sess=['Amigo2','Som2','Dlx1','Thy7']                                  \n",
    "for id,s in zip(article_ids,sess):\n",
    "    datapath = os.path.join(parent_dir,'Downloaded_data', f'{s}')\n",
    "    if os.path.isdir(datapath):\n",
    "        print(f\"{s} session already exists. Moving on.\")\n",
    "    else:\n",
    "        print(\"Downloading data... Please wait, this might take up some time\")        # Can take up to 10 minutes\n",
    "        fshare.retrieve_files_from_article(id,directory=datapath)\n",
    "        print(\"Data downloaded!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data load\n",
    "The training sessions' LFP will be appended together in a list. The same will happen with the ripples detection times.\n",
    "That is the required input for the training parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The training sessions will be appended together. Replace this cell with your own data loading\n",
    "train_LFPs=[]\n",
    "train_GTs=[]\n",
    "# Amigo2\n",
    "path=os.path.join(parent_dir,'Downloaded_data','Amigo2','figshare_16847521')\n",
    "LFP,GT=aux_fcn.load_lab_data(path)\n",
    "train_LFPs.append(LFP)\n",
    "train_GTs.append(GT)\n",
    "# Som2\n",
    "path=os.path.join(parent_dir,'Downloaded_data','Som2','figshare_16856137')\n",
    "LFP,GT=aux_fcn.load_lab_data(path)\n",
    "train_LFPs.append(LFP)\n",
    "train_GTs.append(GT)\n",
    "\n",
    "## Append all your validation sessions\n",
    "val_LFPs=[]\n",
    "val_GTs=[]\n",
    "# Dlx1 Validation\n",
    "path=os.path.join(parent_dir,'Downloaded_data','Dlx1','figshare_14959449')\n",
    "LFP,GT=aux_fcn.load_lab_data(path)\n",
    "val_LFPs.append(LFP)\n",
    "val_GTs.append(GT)\n",
    "# Thy07 Validation\n",
    "path=os.path.join(parent_dir,'Downloaded_data','Thy7','figshare_14960085')\n",
    "LFP,GT=aux_fcn.load_lab_data(path)\n",
    "val_LFPs.append(LFP)\n",
    "val_GTs.append(GT)\n",
    "\n",
    "x_training,GT_training,x_val_list,GT_val_list=rippl_AI.prepare_training_data(train_LFPs,train_GTs,val_LFPs,val_GTs,sf=30000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM training parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters:\n",
    "* Channels:  Number of channels that will be used to train the model, extracted from the data shape defined in the previous cell\n",
    "* Timesteps: Number of samples that the will be used to generate a single prediction\n",
    "* Undersampler proportion: roportion of True/False samples. Using all the samples demands heavy resources usage, but the most data used, the better the model generalizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf=  { 'timesteps':[1,2,4],              # Possible values: 1,2,4,8,16,32... \n",
    "         'undersampler proportion':[1]}    # Possible values: 1, 0.5, 0.1 ... (0,1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desired sampling frequency of the models\n",
    "sf=1250\n",
    "th_arr=np.linspace(0.1,0.9,9)\n",
    "model_name_arr=[]           # To plot in the next cell\n",
    "model_arr=[]                # Actual model array, used in the next validation section\n",
    "n_channels=x_training.shape[1]\n",
    "timesteps_arr=conf['timesteps']\n",
    "undersampler_arr=conf['undersampler proportion']\n",
    "l_ts=len(timesteps_arr)\n",
    "l_us=len(undersampler_arr)\n",
    "n_iters=l_ts*l_us\n",
    "# GT is in the shape (n_events x 2), a y output signal with the same length as x is required\n",
    "perf_train_arr=np.zeros(shape=(n_iters,len(th_arr),3)) # Performance array, (n_models x n_th x 3 ) [P R F1]\n",
    "perf_test_arr=np.zeros_like(perf_train_arr)\n",
    "timesteps_arr_ploting=[]            # Array that will be used in the validation, to be able to call the function predict\n",
    "print(f'{n_channels} will be used to train the SVM models')\n",
    "\n",
    "print(f'{n_iters} models will be trained')\n",
    "\n",
    "x_test_or,GT_test,x_train_or,GT_train=aux_fcn.split_data(x_training,GT_training,split=0.7,sf=sf)\n",
    "\n",
    "y_test_or= np.zeros(shape=(len(x_test_or)))\n",
    "for ev in GT_test:\n",
    "    y_test_or[int(sf*ev[0]):int(sf*ev[1])]=1\n",
    "y_train_or= np.zeros(shape=(len(x_train_or)))\n",
    "for ev in GT_train:\n",
    "    y_train_or[int(sf*ev[0]):int(sf*ev[1])]=1\n",
    "\n",
    "\n",
    "for i_ts,timesteps in enumerate(timesteps_arr):\n",
    "\n",
    "    x_train=x_train_or[:len(x_train_or)-len(x_train_or)%timesteps].reshape(-1,timesteps*n_channels)\n",
    "    y_train_aux=y_train_or[:len(y_train_or)-len(y_train_or)%timesteps].reshape(-1,timesteps)\n",
    "    y_train=aux_fcn.rec_signal(y_train_aux) \n",
    "    \n",
    "    x_test=x_test_or[:len(x_test_or)-len(x_test_or)%timesteps].reshape(-1,timesteps*n_channels)\n",
    "    y_test_aux=y_test_or[:len(y_test_or)-len(y_test_or)%timesteps].reshape(-1,timesteps)\n",
    "    y_test=aux_fcn.rec_signal(y_test_aux)\n",
    "\n",
    "\n",
    "    for i_us,undersampler_prop in enumerate(undersampler_arr):\n",
    "        rus = RandomUnderSampler(sampling_strategy=undersampler_prop)\n",
    "        x_train_us, y_train_us = rus.fit_resample(x_train, y_train)\n",
    "        iter=i_ts*l_us+i_us\n",
    "        print(f\"\\nIteration {iter+1} out of {n_iters}\")\n",
    "        print(f'Time steps: {timesteps}, Undersampler proportion: {undersampler_prop}')\n",
    "        clf = sk.calibration.CalibratedClassifierCV(svm.LinearSVC()) \n",
    "\n",
    "        # Training \n",
    "        clf.fit(x_train_us, y_train_us)\n",
    "        model_arr.append(clf)\n",
    "        # Prediction. One value per window\n",
    "        test_signal = clf.predict_proba(x_test)[:,1]\n",
    "        train_signal=clf.predict_proba(x_train)[:,1]\n",
    "        # Not compatible with the functions that extract beginning and end times\n",
    "        y_train_predict=np.empty(shape=(x_train.shape[0]*timesteps,1,1))\n",
    "        for i,window in enumerate(train_signal):\n",
    "            y_train_predict[i*timesteps:(i+1)*timesteps]=window\n",
    "        \n",
    "        y_test_predict=np.empty(shape=(x_test.shape[0]*timesteps,1,1))\n",
    "        for i,window in enumerate(test_signal):\n",
    "            y_test_predict[i*timesteps:(i+1)*timesteps]=window\n",
    "        \n",
    "        for i,th in enumerate(th_arr):\n",
    "            # Test\n",
    "            ytest_pred_ind=aux_fcn.get_predictions_index(y_test_predict,th)/sf\n",
    "            perf_test_arr[iter,i]=aux_fcn.get_performance(ytest_pred_ind,GT_test,0)[0:3]\n",
    "            # Train\n",
    "            ytrain_pred_ind=aux_fcn.get_predictions_index(y_train_predict,th)/sf\n",
    "            perf_train_arr[iter,i]=aux_fcn.get_performance(ytrain_pred_ind,GT_train,0)[0:3]\n",
    "\n",
    "        # Saving the model\n",
    "        model_name=f\"SVM_Ch{n_channels}_Ts{timesteps:03d}_Us{undersampler_prop:1.2f}\"\n",
    "        \n",
    "        aux_fcn.fcn_save_pickle(os.path.join(parent_dir,'explore_models',model_name),clf)\n",
    "        model_name_arr.append(model_name)\n",
    "        timesteps_arr_ploting.append(timesteps)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training results\n",
    "fig,axs=plt.subplots(n_iters,2,figsize=(10,2*n_iters),sharey='col',sharex='col')\n",
    "\n",
    "for i in range(n_iters):\n",
    "    axs[i,0].plot(perf_train_arr[i,:,0],perf_train_arr[i,:,1],'k.-')\n",
    "    axs[i,0].plot(perf_test_arr[i,:,0],perf_test_arr[i,:,1],'b.-')\n",
    "    axs[i,1].plot(th_arr,perf_train_arr[i,:,2],'k.-')\n",
    "    axs[i,1].plot(th_arr,perf_test_arr[i,:,2],'b.-')\n",
    "    axs[i,0].set_title(model_name_arr[i])\n",
    "    axs[i,0].set_ylabel('Precision')\n",
    "    axs[i,1].set_ylabel('F1')\n",
    "axs[-1,0].set_xlabel('Recall')\n",
    "axs[-1,1].set_xlabel('Threshold')\n",
    "axs[0,0].legend(['Training','Test'])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For loop iterating over the models\n",
    "importlib.reload(rippl_AI)\n",
    "importlib.reload(aux_fcn)\n",
    "fig,axs=plt.subplots(n_iters,2,figsize=(10,2*n_iters),sharey='col',sharex='col')\n",
    "for n_m,model in enumerate(model_arr):\n",
    "    F1_arr=np.zeros(shape=(len(x_val_list),len(th_arr))) #(n_val_sess x n_th) Array where the F1 val of each sesion will be stored\n",
    "    for n_sess,LFP in enumerate(x_val_list):\n",
    "        val_pred=rippl_AI.predict(LFP,sf=1250,arch='SVM',new_model=model,n_channels=n_channels,n_timesteps=timesteps_arr_ploting[n_m])[0]\n",
    "        for i,th in enumerate(th_arr):\n",
    "            val_pred_ind=aux_fcn.get_predictions_index(val_pred,th)/sf\n",
    "            F1_arr[n_sess,i]=aux_fcn.get_performance(val_pred_ind,GT_val_list[n_sess],verbose=False)[2]\n",
    "    \n",
    "    axs[n_m,0].plot(th_arr,perf_train_arr[n_m,:,2],'k.-')\n",
    "    axs[n_m,0].plot(th_arr,perf_test_arr[n_m,:,2],'b.-')\n",
    "    for F1 in F1_arr:\n",
    "        axs[n_m,1].plot(th_arr,F1)\n",
    "    axs[n_m,1].plot(th_arr,np.mean(F1_arr,axis=0),'k.-')\n",
    "    axs[n_m,0].set_title(model_name_arr[n_m])\n",
    "    axs[n_m,0].set_ylabel('Precision')\n",
    "    axs[n_m,1].set_ylabel('F1')\n",
    "axs[-1,0].set_xlabel('Recall')\n",
    "axs[-1,1].set_xlabel('Threshold')\n",
    "plt.show()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PublicBCG_d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
