{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBOOST parameter exploration\n",
    "This notebook is a template for finding the XGBOOST model best suited for your needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "import sys\n",
    "parent_dir=os.path.dirname(os.getcwd())\n",
    "sys.path.insert(0,parent_dir )\n",
    "import rippl_AI\n",
    "import aux_fcn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data download\n",
    "4 uLED sessions will be downloaded: Amigo2 and Som2 will be used for training ; Dlx1 and Thy7 for validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from figshare.figshare.figshare import Figshare\n",
    "fshare = Figshare()\n",
    "\n",
    "article_ids = [16847521,16856137,14959449,14960085] \n",
    "sess=['Amigo2','Som2','Dlx1','Thy7']                                  \n",
    "for id,s in zip(article_ids,sess):\n",
    "    datapath = os.path.join(parent_dir,'Downloaded_data', f'{s}')\n",
    "    if os.path.isdir(datapath):\n",
    "        print(f\"{s} session already exists. Moving on.\")\n",
    "    else:\n",
    "        print(\"Downloading data... Please wait, this might take up some time\")        # Can take up to 10 minutes\n",
    "        fshare.retrieve_files_from_article(id,directory=datapath)\n",
    "        print(\"Data downloaded!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data load\n",
    "The training sessions' LFP will be appended together in a list. The same will happen with the ripples detection times.\n",
    "That is the required input for the training parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The training sessions will be appended together. Replace this cell with your own data loading\n",
    "train_LFPs=[]\n",
    "train_GTs=[]\n",
    "# Amigo2\n",
    "path=os.path.join(parent_dir,'Downloaded_data','Amigo2','figshare_16847521')\n",
    "LFP,GT=aux_fcn.load_lab_data(path)\n",
    "train_LFPs.append(LFP)\n",
    "train_GTs.append(GT)\n",
    "# Som2\n",
    "path=os.path.join(parent_dir,'Downloaded_data','Som2','figshare_16856137')\n",
    "LFP,GT=aux_fcn.load_lab_data(path)\n",
    "train_LFPs.append(LFP)\n",
    "train_GTs.append(GT)\n",
    "\n",
    "## Append all your validation sessions\n",
    "val_LFPs=[]\n",
    "val_GTs=[]\n",
    "# Dlx1 Validation\n",
    "path=os.path.join(parent_dir,'Downloaded_data','Dlx1','figshare_14959449')\n",
    "LFP,GT=aux_fcn.load_lab_data(path)\n",
    "val_LFPs.append(LFP)\n",
    "val_GTs.append(GT)\n",
    "# Thy07 Validation\n",
    "path=os.path.join(parent_dir,'Downloaded_data','Thy7','figshare_14960085')\n",
    "LFP,GT=aux_fcn.load_lab_data(path)\n",
    "val_LFPs.append(LFP)\n",
    "val_GTs.append(GT)\n",
    "\n",
    "x_training,GT_training,x_val_list,GT_val_list=rippl_AI.prepare_training_data(train_LFPs,train_GTs,val_LFPs,val_GTs,sf=30000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBOOST training parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters:\n",
    "* Channels:  number of channels that will be used to train the model, extracted from the data shape defined in the previous cell\n",
    "* Timesteps: number of samples that the will be used to generate a single prediction\n",
    "* Max depth: number of max layers in each tree. Too many usually causes overfitting\n",
    "* Learning rate: similar to a weight used to update te predictor, a high value leads to faster computations but may not reaach a optimal value\n",
    "* Gamma: Minimum loss reduction required to make a partition on a leaf node. The larger gamma is, the more conservative the model will be\n",
    "* Reg lamda: L2 regularization term of weight updating. Increasing this value makes the model more conservative\n",
    "* Scale pos weight: controls the balance of positive and negative weights, useful for unbalanced clasess.\n",
    "* Subsample: subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. Used to prevent overfitting\n",
    "#\n",
    "XGBOOST contains many more parameters, feel free to add your own modifications. Check the oficial documentation: https://xgboost.readthedocs.io/en/stable/parameter.html#parameters-for-tree-booster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf=   {\"timesteps\":[16],          # 2,4,8,16,20,32 ...\n",
    "        \"max_depth\": [4, 5],          # 3,4,5,6,7 ...\n",
    "        \"learning_rate\": [0.1], # 0.2, 0.1, 0.05, 0.01 ...\n",
    "        \"gamma\": [1],                 # 0, 0.25, 1 ...\n",
    "        \"reg_lambda\": [10],           # 0, 1, 10 ...\n",
    "        \"scale_pos_weight\": [1],      # 1, 3, 5...\n",
    "        \"subsample\": [0.8]}           # 0.5, 0.8, 0.9 ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desired sampling frequency of the models\n",
    "sf=1250\n",
    "th_arr=np.linspace(0.1,0.9,9)\n",
    "model_name_arr=[]           # To plot in the next cell\n",
    "model_arr=[]                # Actual model array, used in the next validation section\n",
    "n_channels=x_training.shape[1]\n",
    "timesteps_arr=conf['timesteps']\n",
    "\n",
    "max_depth_arr=conf[\"max_depth\"]        \n",
    "lr_arr=conf[\"learning_rate\"]\n",
    "gamma_arr=conf[\"gamma\"]                                             \n",
    "reg_lambda_arr=conf[\"reg_lambda\"]         \n",
    "scale_arr=conf[\"scale_pos_weight\"]\n",
    "subsample_arr=conf[\"subsample\"]      \n",
    "\n",
    "l_ts=len(timesteps_arr)\n",
    "\n",
    "l_maxd=len(max_depth_arr)\n",
    "l_lr =len(lr_arr)\n",
    "l_g =len(gamma_arr)\n",
    "l_reg =len(reg_lambda_arr)\n",
    "l_sc =len(scale_arr)\n",
    "l_sub =len(subsample_arr)\n",
    "n_iters=l_ts*l_maxd*l_lr*l_g*l_reg*l_sc*l_sub\n",
    "# GT is in the shape (n_events x 2), a y output signal with the same length as x is required\n",
    "perf_train_arr=np.zeros(shape=(n_iters,len(th_arr),3)) # Performance array, (n_models x n_th x 3 ) [P R F1]\n",
    "perf_test_arr=np.zeros_like(perf_train_arr)\n",
    "timesteps_arr_ploting=[]            # Array that will be used in the validation, to be able to call the function predict\n",
    "\n",
    "print(f'{n_channels} channels will be used to train the XGBOOST models')\n",
    "\n",
    "print(f'{n_iters} models will be trained')\n",
    "\n",
    "x_test_or,GT_test,x_train_or,GT_train=aux_fcn.split_data(x_training,GT_training,split=0.7,sf=sf)\n",
    "\n",
    "y_test_or= np.zeros(shape=(len(x_test_or)))\n",
    "for ev in GT_test:\n",
    "    y_test_or[int(sf*ev[0]):int(sf*ev[1])]=1\n",
    "y_train_or= np.zeros(shape=(len(x_train_or)))\n",
    "for ev in GT_train:\n",
    "    y_train_or[int(sf*ev[0]):int(sf*ev[1])]=1\n",
    "\n",
    "\n",
    "for i_ts,timesteps in enumerate(timesteps_arr):\n",
    "\n",
    "        x_train=x_train_or[:len(x_train_or)-len(x_train_or)%timesteps].reshape(-1,timesteps*n_channels)\n",
    "        y_train_aux=y_train_or[:len(y_train_or)-len(y_train_or)%timesteps].reshape(-1,timesteps)\n",
    "        y_train=aux_fcn.rec_signal(y_train_aux) # If any sample of the window contains a ripple, the desired output for the shape is 1\n",
    "        \n",
    "        x_test=x_test_or[:len(x_test_or)-len(x_test_or)%timesteps].reshape(-1,timesteps*n_channels)\n",
    "        y_test_aux=y_test_or[:len(y_test_or)-len(y_test_or)%timesteps].reshape(-1,timesteps)\n",
    "        y_test=aux_fcn.rec_signal(y_test_aux)\n",
    "\n",
    "        for i_maxd,max_depth in enumerate(max_depth_arr):\n",
    "                for i_lr,lr in enumerate(lr_arr):\n",
    "                    for i_g,g in enumerate(gamma_arr):\n",
    "                        for i_rg,reg_l in enumerate(reg_lambda_arr):\n",
    "                            for i_sc,scale in enumerate(scale_arr):\n",
    "                                for i_subs,subsample in enumerate(subsample_arr):\n",
    "                                    iter=(((((i_ts*l_maxd+i_maxd)*l_lr+i_lr)*l_g+i_g)*l_reg+ i_rg)*l_sc + i_sc)*l_sub + i_subs\n",
    "                                    print(f\"\\nIteration {iter+1} out of {n_iters}\")\n",
    "                                    print(f'Number of channels: {n_channels:d}, Time steps: {timesteps:d}.\\nMax depth: {max_depth:d}, Lr: {lr:1.3f}, gamma: {g:1.2f}, reg_l: {reg_l:d}, scale: {scale:1.3f}, subsample: {subsample:0.3f}')\n",
    "                                    xgb = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "                                    colsample_bynode=1, colsample_bytree=1, gamma=g, gpu_id=-1,\n",
    "                                    importance_type='gain', interaction_constraints='',\n",
    "                                    learning_rate=lr, max_delta_step=0, max_depth=max_depth,\n",
    "                                    min_child_weight=1, monotone_constraints='()',\n",
    "                                    n_estimators=100, n_jobs=-1, num_parallel_tree=1, random_state=0,\n",
    "                                    reg_alpha=0, reg_lambda=reg_l, scale_pos_weight=scale, subsample=subsample,\n",
    "                                    tree_method='exact', validate_parameters=1, verbosity=2)\n",
    "\n",
    "                                    # Training\n",
    "                                    xgb.fit(x_train, y_train,verbose=1,eval_metric=[\"logloss\"] ,eval_set = [(x_train,y_train),(x_test, y_test)])\n",
    "                                    model_arr.append(xgb)\n",
    "                                    # Prediction. One value for window\n",
    "                                    test_signal = xgb.predict_proba(x_test)[:,1]\n",
    "                                    train_signal=xgb.predict_proba(x_train)[:,1]\n",
    "                                    # Not compatible with the functions that extract beginning and end times\n",
    "                                    y_train_predict=np.empty(shape=(x_train.shape[0]*timesteps,1,1))\n",
    "                                    for i,window in enumerate(train_signal):\n",
    "                                        y_train_predict[i*timesteps:(i+1)*timesteps]=window\n",
    "                                    \n",
    "                                    y_test_predict=np.empty(shape=(x_test.shape[0]*timesteps,1,1))\n",
    "                                    for i,window in enumerate(test_signal):\n",
    "                                        y_test_predict[i*timesteps:(i+1)*timesteps]=window\n",
    "        \n",
    "                                    for i,th in enumerate(th_arr):\n",
    "                                        # Test\n",
    "                                        ytest_pred_ind=aux_fcn.get_predictions_index(y_test_predict,th)/sf\n",
    "                                        perf_test_arr[iter,i]=aux_fcn.get_performance(ytest_pred_ind,GT_test,0)[0:3]\n",
    "                                        # Train\n",
    "                                        ytrain_pred_ind=aux_fcn.get_predictions_index(y_train_predict,th)/sf\n",
    "                                        perf_train_arr[iter,i]=aux_fcn.get_performance(ytrain_pred_ind,GT_train,0)[0:3]\n",
    "\n",
    "                                    # Saving the model\n",
    "                                    model_name=f\"XGBOOST_Ch{n_channels:d}_Ts{timesteps:03d}_D{max_depth:d}_Lr{lr:1.3f}_G{g:1.2f}_regl{reg_l:02d}_SCALE{scale:03d}_Subs{subsample:1.3f}\"\n",
    "                                    xgb.save_model(os.path.join(parent_dir,'explore_models',model_name))\n",
    "\n",
    "                                    model_name_arr.append(model_name)\n",
    "                                    timesteps_arr_ploting.append(timesteps)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training results\n",
    "fig,axs=plt.subplots(n_iters,2,figsize=(10,2*n_iters),sharey='col',sharex='col')\n",
    "\n",
    "for i in range(n_iters):\n",
    "    axs[i,0].plot(perf_train_arr[i,:,0],perf_train_arr[i,:,1],'k.-')\n",
    "    axs[i,0].plot(perf_test_arr[i,:,0],perf_test_arr[i,:,1],'b.-')\n",
    "    axs[i,1].plot(th_arr,perf_train_arr[i,:,2],'k.-')\n",
    "    axs[i,1].plot(th_arr,perf_test_arr[i,:,2],'b.-')\n",
    "    axs[i,0].set_title(model_name_arr[i])\n",
    "    axs[i,0].set_ylabel('Precision')\n",
    "    axs[i,1].set_ylabel('F1')\n",
    "axs[-1,0].set_xlabel('Recall')\n",
    "axs[-1,1].set_xlabel('Threshold')\n",
    "axs[0,0].legend(['Training','Test'])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For loop iterating over the models\n",
    "fig,axs=plt.subplots(n_iters,2,figsize=(10,2*n_iters),sharey='col',sharex='col')\n",
    "for n_m,model in enumerate(model_arr):\n",
    "    F1_arr=np.zeros(shape=(len(x_val_list),len(th_arr))) #(n_val_sess x n_th) Array where the F1 val of each sesion will be stored\n",
    "    for n_sess,LFP in enumerate(x_val_list):\n",
    "        val_pred=rippl_AI.predict(LFP,sf=1250,arch='XGBOOST',new_model=model,n_channels=n_channels,n_timesteps=timesteps_arr_ploting[n_m])[0]\n",
    "        for i,th in enumerate(th_arr):\n",
    "            val_pred_ind=aux_fcn.get_predictions_index(val_pred,th)/sf\n",
    "            F1_arr[n_sess,i]=aux_fcn.get_performance(val_pred_ind,GT_val_list[n_sess],verbose=False)[2]\n",
    "    \n",
    "    axs[n_m,0].plot(th_arr,perf_train_arr[n_m,:,2],'k.-')\n",
    "    axs[n_m,0].plot(th_arr,perf_test_arr[n_m,:,2],'b.-')\n",
    "    for F1 in F1_arr:\n",
    "        axs[n_m,1].plot(th_arr,F1)\n",
    "    axs[n_m,1].plot(th_arr,np.mean(F1_arr,axis=0),'k.-')\n",
    "    axs[n_m,0].set_title(model_name_arr[n_m])\n",
    "    axs[n_m,0].set_ylabel('Precision')\n",
    "    axs[n_m,1].set_ylabel('F1')\n",
    "axs[-1,0].set_xlabel('Recall')\n",
    "axs[-1,1].set_xlabel('Threshold')\n",
    "plt.show()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PublicBCG_d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
